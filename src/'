using LinearAlgebra
using Distributions
using Random
using Plots
using Optim
using StatsFuns
using DispatchDoctor: @stable

# David's notes: https://www.cs.cornell.edu/courses/cs6241/2025sp/lec/2025-03-11.html
@stable function dist2(x :: AbstractVector{T}, y :: AbstractVector{T}) where {T}
    s = zero(T)
    for k = 1:length(x)
        dk = x[k] - y[k]
        s += dk * dk
    end
    s
end

@stable dist(x :: AbstractVector{T}, y :: AbstractVector{T}) where {T} = sqrt(dist2(x,y))

abstract type KernelContext end
(ctx :: KernelContext)(arg ... ) = kernel(ctx, args ... )

abstract type RBFKernelContext{d} <: KernelContext end

ndims(::RBFKernelContext{d}) where {d} = d

# squared exponential kernel.
function Dφ_SE(s :: Float64)
    φ = exp(-s^2/2)
    dφ_div = -φ
    dφ = dφ_div*s
    Hφ = (-1 + s^2)*φ
    φ, dφ_div, dφ, Hφ
end

φ_SE(s::Float64) = exp(-s^2/2)

struct KernelSE{d} <: RBFKernelContext{d}
    ℓ :: Float64
end
Dφ(::KernelSE, s) = φ_SE
nhypers(::KernelSE) = 1
$(esc(:getθ!))(θ, ctx :: $T) = θ[1]=ctx.ℓ
$(esc(:updateθ))(ctx :: $T{d}, θ) where {d} = $T{d}(θ[1])

@rbf_simple_kernel(KernelSE, ϕ_SE, Dϕ_SE)

function getθ(ctx :: KernelContext)
    θ = zeros(nhypers(ctx))
    getθ!(θ, ctx)
    θ
end

kernel(ctx :: RBFKernelContext, x :: AbstractVector, y :: AbstractVector) =
    φ(ctx, dist(x, y)/ctx.l)
# gradients

# wrt x1; fix x2 
function k_g!(storage::AbstractVector, x1::AbstractVector, x2::AbstractVector)
    storage = k(x1, x2, gp.lengthscale, gp.variance) * -1/(gp.lengthscale^2) * x1
end

# Julia docs Constructors: It is good practice to provide as few inner constructor methods as
# possible: only those taking all arguments explicitly and enforcing essential error checking
# and transformation. Additional convenience constructor methods, supplying default values or
# auxiliary transformations, should be provided as outer constructors that call the inner
# constructors to do the heavy lifting. This separation is typically quite natural.
struct GP
    X::Matrix{Float64}
    y::Vector{Float64}
    kernel::Function
    L::Matrix{Float64}
    lengthscale::Float64
    variance::Float64
    # check the sizes on X and y
    # check that size of L matches X
    function GP(X::Matrix{Float64}, y::Vector{Float64}, L::Matrix{Float64},
                kernel::Function, lengthscale::Float64, variance::Float64)
        return new(X, y, kernel, L, lengthscale, variance)
    end

    function GP(X::Matrix{Float64}, y::Vector{Float64}, kernel::Function,
            lengthscale::Float64, variance::Float64)
        K = kernel_matrix_compute(X, X, kernel, lengthscale, variance)
        L = cholesky(K).L
        return new(X, y, kernel, L, lengthscale, variance)
    end
end

function predict(gp::GP, x_new::VecOrMat{Float64})
    K_inv = gp.L' \ (gp.L \ gp.y)
    K21 = kernel_matrix_compute(x_new, gp.X, gp.kernel, gp.lengthscale, gp.variance)
    K22 = kernel_matrix_compute(x_new, x_new, gp.kernel, gp.lengthscale, gp.variance)
    mu = K21 * K_inv
    v = gp.L \ K21'
    sigma2 = K22 - v' * v

    return mu, diag(sigma2)
end

function kernel_matrix_compute(X1::VecOrMat{Float64}, X2::VecOrMat{Float64},
k::Function, lengthscale::Float64, variance::Float64)
    n1 = size(X1, 2)
    n2 = size(X2, 2)
    K = zeros(n1, n2)
    for i in 1:n1
        for j in 1:n2
            K[i, j] = k(X1[:, i], X2[:, j], lengthscale, variance)
        end
    end

    return K
end

function logEI(z::Float64)
    function DψNLG0(z)
        φz = normpdf(z)
        Qz = normccdf(z)
        Gz = φz - z*Qz
        ψz = -log(Gz)
        dψz = Qz/Gz
        Hψz = (-φz*Gz + Qz^2)/Gz^2
        ψz, dψz, Hψz
    end

    function DψNLG2(z)
        # Approximate W by 20th convergent
        W = 0.0
        for k = 20:-1:1
            W = k/(z + W)
        end
        ψz = log1p(z/W) + 0.5*(z^2 + log(2π))
        dψz = 1/W
        Hψz = (1 - W*(z + W))/W^2
        ψz, dψz, Hψz
    end
    DψNLG(z) = if z < 6.0 DψNLG0(z) else DψNLG2(z) end
end

function get_Copt(gp :: GP)
end

function mean(gp :: GP, x :: AbstractVector)

end

function gx_mean(gp :: GP, x :: AbstractVector)
end

function Hx_mean(gp :: GP, x :: AbstractVector)
end

function var(gp :: GP, x :: AbstractVector)
end

function gx_var(gp :: GP, x :: AbstractVector)
end

function Hx_var(gp :: GP, x :: AbstractVector)
end

function Hgx_αNLEI(gp :: GP, x :: AbstractVector, y_best :: Float64)
    Copt = getCopt(gp)
    μ, gμ, Hμ = mean(gp, x), gx_mean(gp, x), Hx_mean(gp, x)
    v, gv, Hv = Copt*var(gp, x), Copt*gx_var(gp, x), Cop*Hx_var(gp, x)

    σ = sqrt(v)
    gμs, Hμs = gμ/σ, Hμ/σ
    gvs, Hvs = gv/(2v), Hv/v
    
    u = (μ - y_best)/σ
    ψ, dψ, Hψ = logEI(u)

    α = -log(σ) + ψ
    dα = dψ*gμs - (1 + u*dψ)*gvs
    Hα = -0.5*(1.0 + u*dψ)*Hvs + dψ*Hμs + Hψ*gμs*gμs' + (2.0 + u^2*Hψ + 3.0*u*dψ)*gvs*gvs'
    -(u*Hψ + dψ)*(gμs*gvs' + gvs*gμs')

    α, dα, Hα
end

@stable function optimize_EI(gp::GP, x_current :: AbstractVector, lo :: AbstractVector,
        hi :: AbstractVector) 
    y_best = minimum(gety(gp))
    fun(x) = Hgx_αNLEI(gp, x, y_best)[1]
    fun_grad!(g, x) = copyto!(g, Hgx_αNLEI(gp, x, y_best)[2])
    fun_hess!(g, x) = copyto!(g, Hgx_αNLEI(gp, x, y_best)[3])
    df = TwiceDifferentiable(fun, fun_grad!, fun_hess!, x0)
    dfc = TwiceDifferentiableConstraints(lo, hi)
    res = optimize(df, dfc, x0, IPNewton())
end

@stable function acquire_next_point(gp::GP, lo :: AbstractVector, hi :: AbstractVector;
        nstarts = 10, verbose=true)
    y_next = Inf
    x_next = [0.0; 0.0]
    for j = 1:10
        z = lo + (hi-lo).*rand(length(lo))
        res = optimize_EI(gp, z, [0.0; 0.0], [1.0; 1.0])
        if verbose
            println("From $z: $(Optim.minimum(res)) at $(Optim.minimizer(res))")
        end
        if Optim.minimum(res) < bestα
            y_next = Optim.minimum(res)
            x_next[:] = Optim.minimizer(res)
        end
    return x_next, y_next
end

function surrogate_model_update!(gp::GP, x::Vector{Float64}, y::Float64)
    X_new = [gp.X x]
    y_new = [gp.y; y]
    n_points = size(X_new, 2)
    
    if n_points % 5 == 0
        lengthscale_new, variance_new = optimize_hyperparameters(X_new, y_new,
        gp.kernel, gp.lengthscale, gp.variance)

        K = kernel_matrix_compute(X_new, X_new, gp.kernel, lengthscale_new, variance_new)
        L_new = cholesky(K + 1e-10*I(size(K,1))).L
        L_new = Matrix(L_new)
    else
        K12 = kernel_matrix_compute(gp.X, reshape(x, :, 1), gp.kernel, gp.lengthscale,
        gp.variance)
        K22 = kernel_matrix_compute(reshape(x, :, 1), reshape(x, :, 1), gp.kernel,
        gp.lengthscale, gp.variance)
        
        L12 = gp.L \ K12
        L22 = sqrt(K22 - L12' * L12) .+ 1e-6
        L_new = [gp.L zeros(Float64, size(gp.L, 1), 1); 
                    reshape(L12', 1, :) L22]
        
        lengthscale_new = gp.lengthscale
        variance_new = gp.variance
    end
    return GP(X_new, y_new, L_new, gp.kernel, lengthscale_new, variance_new)
end

function optimize_hyperparameters(X, y, kernel, lengthscale, variance; method=LBFGS())
    initial_params = [lengthscale, variance]
    lower_bounds = [1e-6, 1e-6]
    upper_bounds = [10.0, 10.0]

    function objective(params)
        return -log_marginal_likelihood(X, y, kernel, params)
    end

    result = Optim.optimize(objective, lower_bounds, upper_bounds, initial_params,
    Fminbox(method))
    opt_params = Optim.minimizer(result)
    opt_lengthscale = opt_params[1]
    opt_variance = opt_params[2]
    
    return opt_lengthscale, opt_variance
end

function log_marginal_likelihood(X::Matrix{Float64}, y::Vector{Float64}, kernel::Function,
        params::Vector{Float64})
    lengthscale = params[1]
    variance = params[2]
    K = kernel_matrix_compute(X, X, kernel, lengthscale, variance)
    n = size(K, 1)
    L = cholesky(K + 1e-6*I(size(K,1))).L
    α = L' \ (L \ y)
    
    return -0.5 * dot(y, α) - sum(log.(diag(L))) - 0.5 * n * log(2π)
end

function BO_loop(f::Function, bounds::Matrix{Float64}, n_iterations::Int; n_init::Int=5)
    dimensions = size(bounds, 1)
    lengthscale = 0.5
    variance = 1.2

    X_init = zeros(dimensions, n_init)
    for d in 1:dimensions
        X_init[d, :] = bounds[1, d] .+ (bounds[2, d] - bounds[1, d]) * rand(n_init)
    end
    y_init = [f(X_init[:, i]) for i in 1:n_init]

    gp = GP(X_init, y_init, rbf_kernel, lengthscale, variance)

    X_all = copy(X_init)
    y_all = copy(y_init)
    y_best = minimum(y_init)
    x_best = X_init[:, argmin(y_init)]

    history = [(x_best, y_best)]
    x_current = x_best

    for i in 1:n_iterations
        x_current, ei = acquire_next_point(gp, x_current, y_best)
        y_current = f(x_current)
        gp = surrogate_model_update!(gp, x_current, y_current)
        X_all = [X_all x_current]
        y_all = [y_all; y_current]
        if y_current < y_best
            y_best = y_current
            x_best = x_current
            push!(history, (x_best, y_best))
        end

        println("Iteration $i: x = $x_current, f(x) = $y_current, Best = $y_best")
    end
    lengthscale_final = gp.lengthscale
    variance_final = gp.variance
    println("Final lengthscale $lengthscale_final, final variance $variance_final")

    return x_best, y_best, history, gp
end
